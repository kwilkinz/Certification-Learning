# Version 2 Study Guide Questions 

## Fundamentals of Large Language Models (LLMs) - 8 Questions 

**Q1. What distigueses large language models (LLMs) from traditional models?**
- A) Limited vocabulary
- B) Enhanced context understanding
- C) Ability to generate images
- D) Dependency on extensive manual labeling

**Q2. How do large language models (LLMs) primarily contribute to AI applications?**
- A) By improving hardware efficiency
- B) By optimizing database management
- C) By generating human-like text 
- D) By enabling physical object reconition

**Q3. Which neural network architecture is foundational for large language models (LLMs)?**
- A) Convolutional Neural Networks (CNNs)
- B) Recurrent Neural Networkd (RNNs)
- C) Transformers 
- D) Long Short-Term Memory (LTSM) Networks

**Q4. What role does the attention mechanism serve in Transformer models?**
- A) Enhancing image resolution
- B) Focusing on relevant input elements 
- C) Optimizing memory utilization
- D) Reducing computation complexitiy 

**Q5. How are prompts utilized in large language models (LLMs)?**
- A) As model architectures
- B) As neural network layers
- C) As input instructions or queries
- D) As training algorithms

**Q6: What practice enhances the effectiveness of prompts in LLM applications?**
- A) Using ambiguous language
- B) Incorporating random noise
- C) Providing context-specific details
- D) Avoiding structured queries

**Q7: What is the primary objective of fine-tuning a large language model (LLM)?**
- A) To increase model complexity
- B) To adapt the model to specific tasks or datasets
- C) To reduce dataset size requirements
- D) To modify the model's neural architecture

**Q8: Which model type integrates both textual and visual information?**
- A) Text-only models
- B) Multi-modal models
- C) Code models
- D) Sequential models

### Using OCI Generative AI Service - 18 Questions 

**Q9: Which OCI service is dedicated to text generation tasks?**
- A) OCI Data Integration
- B) OCI Generative AI Service 
- C) OCI Compute
- D) OCI Storage

**Q10: What purpose does text summarization serve in AI applications?**
- A) Generating detailed reports
- B) Extracting essential information 
- C) Converting speech to text
- D) Enhancing image clarity

**Q11: Why are dedicated AI clusters essential in OCI?**
- A) For managing user permissions
- B) For efficient model optimization 
- C) For handling graphical data
- D) For deploying web applications

**Q12: In the context of AI models, what does inference involve?**
- A) Training the model 
- B) Assessing model performance
- C) Data preprocessing
- D) Generating predictions

**Q13: What prerequisites are necessary for fine-tuning an LLM in OCI?**
- A) A pre-trained base model 
- B) A dedicated AI cluster
- C) A specialized software license
- D) All of the above

**Q14: What defines a model endpoint in OCI?**
- A) A platform for accessing datasets
- B) A RESTful API for model interaction 
- C) An interface for visualizing AI output
- D) A tool for network monitoring

**Q15: Why is securing model endpoints crucial in AI deployments?**
- A) To enhance model adaptability
- B) To ensure data confidentiality and integrity 
- C) To optimize computational resources
- D) To simplify debugging processes

**Q16: What characterizes OCI's security architecture for AI services?**
- A) Absence of encryption protocols
- B) Role-based access control (RBAC) 
- C) Unlimited network accessibility
- D) Manual data transmission

**Q17: How does OCI ensure secure data transmission?**
- A) Through HTTP protocols
- B) Via FTP connections
- C) Using HTTPS encryption 
- D) With Telnet-based transfers

**Q18: Which model type supports both textual and visual data processing?**
- A) Text-based models
- B) Code models
- C) Multi-modal models 
- D) Language agents

### Building an LLM Application with OCI Generative AI Service - 14 Questions

**Q19: What does RAG stand for in AI applications?**
- A) Rapid Algorithm Generation
- B) Retrieval Augmented Generation 
- C) Recursive AI Growth
- D) Redundant Algorithmic Generation

**Q20: How does RAG enhance LLM applications?**
- A) By minimizing dataset requirements
- B) By optimizing algorithmic complexity
- C) By improving content relevance and accuracy 
- D) By automating model training

**Q21: What role does a vector database play in AI applications?**
- A) Storing text in binary format
- B) Managing complex mathematical computations
- C) Storing embeddings for efficient retrieval 
- D) Enabling data encryption

**Q22: Which feature is critical in a vector database for efficient search?**
- A) String-based indexing 
- B) Fast similarity search 
- C) Complex data representation
- D) Regular expression validation

**Q23: What defines semantic search in AI contexts?**
- A) Keyword-based retrieval
- B) Contextual query matching 
- C) Image-based retrieval
- D) Metadata-centric search

**Q24: How does LangChain facilitate LLM application development?**
- A) By reducing programming complexity 
- B) By focusing on database management
- C) By integrating external APIs
- D) By automating model evaluation

**Q25: In LangChain, what is the primary function of a chain?**
- A) Sequencing data preprocessing steps 
- B) Orchestrating data visualization tasks
- C) Enabling network encryption
- D) Supporting model deployment

**Q26: Which step is integral to building an LLM application with RAG and LangChain?**
- A) Defining input prompts and data chains
- B) Training from scratch without pre-existing models
- C) Avoiding reliance on vector databases
- D) Omitting retrieval mechanisms

**Q27: What is the purpose of tracing an LLM application?**
- A) To improve algorithmic efficiency
- B) To debug data flow and prompts 
- C) To optimize hardware performance
- D) To secure model endpoints

**Q28: Which tool is suitable for evaluating LLM application performance?**
- A) Code compiler
- B) Performance monitoring tools 
- C) Image processing libraries
- D) File management utilities

**Q29: What is a crucial step in deploying an LLM application on OCI?**
- A) Implementing complex security configurations 
- B) Establishing model endpoints for inference 
- C) Limiting resource allocation
- D) Disabling network connectivity

**Q30: Why is continuous monitoring essential post-deployment of an LLM application?**
- A) To enforce output consistency
- B) To promptly address performance issues 
- C) To minimize data storage costs
- D) To avoid software updates

**Q31: What primary factor influences scaling of an LLM application?**
- A) Fixed resource allocation
- B) Dynamic resource management based on usage 
- C) Limited user access
- D) Reduced model complexity

**Q32: Which OCI service feature ensures secure model deployment?**
- A) Open public access
- B) Encrypted data storage and transmission 
- C) Limited resource availability
- D) Fixed data processing protocols

**Q33: How does identity and access management (IAM) impact OCI AI services?**
- A) Enhancing data visualization capabilities
- B) Managing resource access permissions
- C) Optimizing model training duration
- D) Enabling real-time data processing

**Q34: What measures ensure OCI compliance with data privacy regulations?**
- A) Storing data in plain text
- B) Implementing stringent security protocols
- C) Disabling all external communications
- D) Restricting model evaluation tools

**Q35: What is a primary benefit of fine-tuning a pre-trained model with a custom dataset?**
- A) It enhances model interpretability
- B) It tailors the model to specific tasks and boosts performance
- C) It automates GPU resource allocation
- D) It simplifies model validation processes

**Q36: What factors should be considered when setting up AI clusters in OCI?**
- A) CPU and memory requirements
- B) GPU capabilities, memory, and storage needs 
- C) Network latency parameters
- D) Software compatibility issues

---

## Answer Key 

**Fundamentals of Large Language Models (LLMs) - 8 Questions**
Q1: B) Enhanced context understanding *
Q2: C) Ability to generate human-like text *
Q3: C) Transformers *
Q4: B) Focusing on relevant parts of the input sequence *
Q5: C) An input query or instruction *
Q6: B) Providing context-specific details *
Q7: B) To adapt the model to specific tasks or datasets *
Q8: B) Multi-modal models *

**Using OCI Generative AI Service - 18 Questions**
Q8: B) OCI Generative AI Service (*)
Q10: B) Extracting essential information (*)
Q11: B) For efficient model optimization (*)
Q12: D) Generating predictions (*)
Q13: D) All of the above (*)
Q14: B) A RESTful API for model interaction (*)
Q15. B) To ensure data confidentiality and integrity (*)
Q16. B) Role-based access control (RBAC) (*)
Q17. C) Using HTTPS encryption (*)
Q18. C) Multi-modal models (*)

**Building an LLM Application with OCI Generative AI Service - 14 Questions**
Q19. B) Retrieval Augmented Generation (*)
Q20. C) By improving content relevance and accuracy (*)
Q21. C) Storing embeddings for efficient retrieval (*)
Q22. B) Fast similarity search (*)
Q23. B) Contextual query matching (*)
Q24. D) By automating model evaluation (*)
Q25. A) Sequencing data preprocessing steps (*)
Q26. A) Defining input prompts and data chains (*) 
Q27. B) To debug data flow and prompts (*)
Q28. B) Performance monitoring tools (*)
Q29. B) Establishing model endpoints for inference (*)
Q30. B) To promptly address performance issues (*)
Q31. B) Dynamic resource management based on usage (*)
Q32. B) Encrypted data storage and transmission (*)
Q33. B) Managing resource access permissions (*)
Q34. B) Implementing stringent security protocols (*)
Q35. B) It tailors the model to specific tasks and boosts performance (*)
Q36. B) GPU capabilities, memory, and storage needs (*)








